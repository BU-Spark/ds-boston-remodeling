{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data From Property Assessment  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Database id by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingbowang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import data.sqlDataFetch as sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_id = sdf.get_id_by_year(2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe The Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of each database have diff key value, I can not use for loop to get key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name          | Type      \n",
      "--------------------------------\n",
      "_id                  | int       \n",
      "PID                  | text      \n",
      "CM_ID                | text      \n",
      "ST_NUM               | text      \n",
      "ST_NAME              | text      \n",
      "ST_NAME_SFX          | text      \n",
      "UNIT_NUM             | text      \n",
      "ZIPCODE              | text      \n",
      "PTYPE                | text      \n",
      "LU                   | text      \n",
      "OWN_OCC              | text      \n",
      "OWNER FY04           | text      \n",
      "MAIL_ADDRESS         | text      \n",
      "MAIL_CITY_STATE      | text      \n",
      "MAIL_ZIP             | text      \n",
      "LOTSIZE              | text      \n",
      "GROSS_AREA           | text      \n",
      "LIVING _AREA         | text      \n",
      "FY2004_TOTAL         | text      \n",
      "FY200_ LAND          | text      \n",
      "FY2004_BLDG          | text      \n",
      "GROSS_TAX            | text      \n",
      "NUM_FLOORS           | text      \n"
     ]
    }
   ],
   "source": [
    "sdf.describe_database(database_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Con_unit, Res_unit, Rc_unit for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "    \"ZIPCODE\"::text AS ZIPCODE,\n",
      "    SUM(\"LIVING_AREA\":: FLOAT) as TOTAL_LIVING_AREA\n",
      "FROM\n",
      "    \"5bfe4ca0-71c0-4751-bdcf-dad4d58445e0\"\n",
      "GROUP BY\n",
      "    \"ZIPCODE\"::text\n",
      "\n",
      "Failed to fetch data: 409\n"
     ]
    }
   ],
   "source": [
    "url = \"https://data.boston.gov/api/3/action/datastore_search_sql\"\n",
    "# Construct the SQL query to select distinct city names from the table\n",
    "sql_query = f\"\"\"\n",
    "SELECT\n",
    "    \"ZIPCODE\"::text AS ZIPCODE,\n",
    "    SUM(\"LIVING_AREA\":: FLOAT) as TOTAL_LIVING_AREA\n",
    "FROM\n",
    "    \"{database_id}\"\n",
    "GROUP BY\n",
    "    \"ZIPCODE\"::text\n",
    "\"\"\"\n",
    "print(sql_query)\n",
    "params = {\"sql\": sql_query}\n",
    "\n",
    "# Send the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if there is data in the response\n",
    "    if data['success'] and 'result' in data and 'records' in data['result']:\n",
    "        records = data['result']['records']\n",
    "        # Filter records with ZIP code length greater than 1 and ensure ZIPCODE is not None\n",
    "        filtered_records = [record for record in records if record['zipcode'] and len(record['zipcode']) > 1]\n",
    "\n",
    "        # Convert the filtered records into a DataFrame\n",
    "        df = pd.DataFrame(filtered_records)\n",
    "    else:\n",
    "        print(\"No data found or error in response.\")\n",
    "else:\n",
    "    print(\"Failed to fetch data:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_zip_map = {\n",
    "    'Allston/Brighton': ['02134', '02135', '02163'],\n",
    "    'Back Bay/Beacon Hill': ['02108', '02116', '02117', '02123', '02133', '02199', '02216', '02217', '02295'],\n",
    "    'Central Boston': [\n",
    "        '02101', '02102', '02103', '02104', '02105', '02106', '02107', '02109', '02110', '02111',\n",
    "        '02112', '02113', '02114', '02196', '02201', '02202', '02203', '02204', '02205', '02206',\n",
    "        '02207', '02208', '02209', '02211', '02212', '02222', '02293'\n",
    "    ],\n",
    "    'Charlestown': ['02129'],\n",
    "    'Dorchester': ['02122', '02124', '02125'],\n",
    "    'East Boston': ['02128', '02228'],\n",
    "    'Fenway/Kenmore': ['02115', '02215'],\n",
    "    'Hyde Park': ['02136'],\n",
    "    'Jamaica Plain': ['02130'],\n",
    "    'Mattapan': ['02126'],\n",
    "    'Roslindale': ['02131'],\n",
    "    'Roxbury': ['02119', '02120', '02121'],\n",
    "    'South Boston': ['02127', '02210', '02219'],\n",
    "    'South End': ['02118'],\n",
    "    'West Roxbury': ['02132'],\n",
    "    'Dedham':['02026', '02137'],\n",
    "    ' Brookline':['02445', '02446', '02467', '02146'],\n",
    "    'Newton':['02458'],\n",
    "    ' Hingham':['02018'],\n",
    "    'Milton':['02186'],\n",
    "    'Westwood':['02090']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zipcode'] = df['zipcode'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zipcode'] = df['zipcode'].str.rstrip('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_neighborhood_map = {zip_code: neighborhood for neighborhood, zip_codes in neighborhood_zip_map.items() for zip_code in zip_codes}\n",
    "\n",
    "df['neighborhood'] = df['zipcode'].map(zip_neighborhood_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zipcode  total_living_area  neighborhood\n",
      "0    01116             1291.0           NaN\n",
      "1    01224             1470.0           NaN\n",
      "2    01473              601.0           NaN\n",
      "3    01720             1353.0           NaN\n",
      "4    01749              540.0           NaN\n",
      "..     ...                ...           ...\n",
      "76   02127             1325.0  South Boston\n",
      "77   02131              591.0    Roslindale\n",
      "78   03070                0.0           NaN\n",
      "79   98221              789.0           NaN\n",
      "80   0009R                0.0           NaN\n",
      "\n",
      "[81 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#df['zipcode'] = df['zipcode'].str.replace('_', '', regex=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save each cvs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = './data/cleaned/'\n",
    "file_name = '2009.csv'\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "  os.makedirs(save_path)\n",
    "\n",
    "df.to_csv(full_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义保存路径\n",
    "save_path = './data/cleaned/'\n",
    "\n",
    "# 定义需要读取的年份范围\n",
    "years = list(range(2009, 2024))  # 生成从2009到2023的列表\n",
    "years.remove(2014)  # 从列表中移除2014\n",
    "years.append(2024)  # 添加2024\n",
    "\n",
    "# 初始化一个空的DataFrame用于存储合并后的数据\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# 遍历每一个年份，读取相应的CSV文件\n",
    "for year in years:\n",
    "    file_name = f'{year}.csv'\n",
    "    full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "    # 读取CSV文件\n",
    "    temp_df = pd.read_csv(full_path)\n",
    "\n",
    "    # 添加一个新列用于标记年份\n",
    "    temp_df['Year'] = year\n",
    "\n",
    "    # 将当前年份的数据添加到合并后的DataFrame中\n",
    "    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "# 保存合并后带有年份标记的数据到新的CSV文件\n",
    "output_file_name = 'combined_2009_2024.csv'\n",
    "output_full_path = os.path.join(save_path, output_file_name)\n",
    "combined_df.to_csv(output_full_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
